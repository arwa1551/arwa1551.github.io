---
layout: post
title: Blockhcian Tweet Analysis
---




![](images/word_cloud.png)

### Intoduction 
Unlike distributed databases, where data is distributed but managed and controlled by one single entity, blockchains allow for distributed control. Different people and institutions, that do not trust each other, share information without requiring a central administrator. It's clear blockchain technology has exciting applications in procurement and the potential to truly revolutionize many different supply chains the world over, and since Twitter has been the primary source of information nowadays, my goal was to discover the most popular topics associted with the word 'Blockchain' in the hope to unfold the next Bitcoin.



### Data acquisition
I used Twitter API to get my data. I managed to fetch around 22,000 tweets with the search word "Blockchain" excluding retweets.

### Data Pre-processing 
* Removed URLs.
* Made text lowercase, removed text in square brackets, removed punctuation and removed words containing numbers.
* Dropped duplicates, which resulted in removing 5009 tweets. 

### Vectorization
I cannot work with text directly when using machine learning algorithms. Instead, I need to convert the text to numbers.
A simple and effictive model for thinking about text documents in machine learing is called the Bag-of-Words Model, or BoW.

Any document we see can be encoded as a fixed-length vector with the length of the vocabulary of known words. The value in each position in the vector could be filled with a count or frequency of each word in the encoded document.

I used CountVectorizer from scikit-learn library, which provides simple way to tokenize and vecorize text documents. 
The vectors returned from a call to transform() will be sparse vectors, and they can be transformed back to numpy arrays to look and better understand what is going on by calling the toarray() function.

### Topic Modeling

#### Non-Negative Matrix Factorization (NMF)
Nonnegative Matrix Factorization (NMF) is a popular dimension reduction technique of clustering by extracting latent features from high-dimensional data.

NMF will take the transpose of the bag-of-words matrix as A and will produce two matrices W and H such that A= WH, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect.

W and H represent the following information about A:

A (Document-word matrix) — input that contains which words appear in which documents.

W (Basis vectors) — the topics (clusters) discovered from the documents.

H (Coefficient matrix) — the membership weights for the topics in each document.

With NMF, one must choose a priori the number of desired topics. The aim of NMF is to find low-dimensional W and H with all non-negative elements minimizing reconstruction error. For that, I compared reconstruction errors for different parameters. (see the plot below). 

![](images/nmf.png)

Even though two topics gives the maximum reconstruction error I went with it because the topics made sense to me! 

The top 30 words for topic #  0
<div class="message">
['ico', 'ai', 'btc', 'binance', 'facebooks', 'tech', 'says', 'finance', 'research', 'work', 'exchanges', 'ibm', 'trade', 'market', 'blockchainbitcoin', 'ethereum', 'world', 'bank', 'digital', 'facebook', 'fintech', 'amp', 'platform', 'industry', 'exchange', 'trading', 'libra', 'herewego', 'cryptocurrency', 'crypto']
</div>

The top 30 words for topic #  1
<div class="message">
['wallet', 'time', 'swapzilla', 'ripple', 'digital', 'like', 'biggest', 'congressman', 'cryptocurrency', 'transaction', 'network', 'usd', 'cryptonews', 'blockchainnews', 'amp', 'fintech', 'qash', 'trading', 'token', 'bitcoinnews', 'market', 'gold', 'ico', 'bch', 'xrp', 'eth', 'price', 'ethereum', 'btc', 'bitcoin'] 
</div>

I interpreted the first topic as News about Blockhcain and the second as Cryptocurrency. 

### Clustering

#### K-means clustering
Just like NMF number of clusters K must be known in advance. the most common approach for choosing the optimal number of clusters is the elbow method. 

For each k value, we will initialise k-means and use the inertia attribute to identify the sum of squared distances of samples to the nearest cluster centre. As k increases, the sum of squared distance tends to zero.

Below is a plot of sum of squared distances for k in the range 1-9. If the plot looks like an arm, then the elbow on the arm is optimal k. As you can see in the plot below, there is a slight bend in 2, 3 and 4 clusters. I went with 4.

![](images/elbow_method.png)

Here, is the distribution of topics in each cluster
<div class="message">
(0: 13068) 
  
(2: 2082)

(1: 1097)

(3: 750)
</div>

Below is the scatter plot of the 4 clusters 

![](images/k_means.png)

Below is a clear representation of the clusters using UMAP

![](images/umap.png)

##### The distrubution of topics in each cluster: 
Clsuter 1: 
It has 77% topic 1 and 75% topic 2

Clsuter 2: 
It has 15% topic 2 

Clsuter 3: 
It has 20% topic 1 

Clsuter 4: 
It has 8.7% topic 2

