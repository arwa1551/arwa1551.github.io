---
layout: post
title: Project 3: Cardiovascular disease prediction
---

# Project 3: Cardiovascular disease prediction

### Introduction
Cardiovascular diseases result in millions of deaths around the globe annually, most of which are avoidable if identified early. Machine learning classification techniques can significantly benefit the medical field by providing an accurate and quick diagnosis of diseases.

The intuition for this classificaltion is identify weather a patient is tested positive or negative given the diagnostic measurement presented in the dataset.

### Dataset Description 
I imported [Cardiovascular Disease dataset](https://www.kaggle.com/sulianova/cardiovascular-disease-dataset) from Kaggle. 
The dataset consists of 70 000 records of patients data in 12 features, such as age, gender, systolic blood pressure, diastolic blood pressure, and etc. The target class "cardio" equals to 1, when patient has cardiovascular desease, and it's 0, if patient is healthy.

### EDA
first step, is to take a look on the features and their values. 
![](images/data_des.PNG)

All data types in this dataset are numerical, so I didn't have to use dummy variables or one hot encoding. 
![](images/info.PNG)

Since the data entires for the age column are given in days, I converted them into years to better understand these values.
As you can see from the plot below, people older than 55 are pron to having CVD. 
![](images/age_count.png)

Since gender is expresed by 1 and 2, I wanted to distinguish which refers to women and which to men.
To do so, I calculated the mean height per gender and based my assutimation on the fact that the mean height for men would be higher than women. 
So I assume that 1 stands for women and 2 for men. 
![](images/mean_height.PNG)

I wanted to know the percentage of population per gender. 
Women cover 65% and men 34%
![](images/gender.PNG)

Most imoportantly, I wnated to make sure that my target class distribution is balanced, it is fairly balanced.
![](images/class.png)

### Data Cleaning
Before I dive into the cleaning part, lets have a little backstory on blood pressure readings. 

Blood pressure is recorded as two numbers:
* Systolic blood pressure (the first number) – indicates how much pressure your blood is exerting against your artery walls when the heart beats.
* Diastolic blood pressure (the second number) – indicates how much pressure your blood is exerting against your artery walls while the heart is resting between beats.

Reference: [American Heart Association (AHA)](https://www.heart.org/en/health-topics/high-blood-pressure/understanding-blood-pressure-readings)

![](images/boold_pressure.gif)

As a result, I removed outliers, such as:
- Removing cases where Diastolic pressure is greater than Systolic.
- Removing negative blood pressure 
- Removing systolic blood pressure outside the range of (70 - 190 ), and diastolic outside the range (40 - 100 ).

### Feature Selection
In order to reduce the complexity of the model I used [mutual_info_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html) from sklearn: which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency. 

As you can see from the below plot, Systolic blood pressure is the most dependent, where's smoking and alcohol are the least. As a result, I discarded those two features, since their presence won't contribute to have a better model classification. 
![](images/feature_impo.png)

However, I was a little concern, because from my previous knowledge, these two features should have an impact on weather the patient has CVD or not! There must be a difference between patients who smoke and consume alcohol and patients who dont.  


<div class="message">
  Alcohol and tobacco use both have important effects on cardiovascular risk factors. 
</div>


Reference: [Alcohol Research: Current Reviews](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6527044/)

Then, I discovered that the problem occured because of these features distributions. As you can see from the plots below 94% of the population don't consume alcohol. The same for smoking, where 91% are not smokers. That eventually left unbalance and unfaire comparison that resulted in having the least dependence between these two features and the target class. 

![](images/smoke_p.PNG) 
![](images/smoke_plt.PNG) 

![](images/alco_p.PNG)
![](images/alco_plt.PNG)

### Feature Engineering
After splitting the data into training and testing, I did some feature engineering including multi-interaction term where I calculated the BMI from the height and weight features, and I did power transform on this new feature. Then, I used StandardScaler such that all my data distribution will have a mean value 0 and standard deviation of 1.


### Modeling 
I used GridSearchCV with pipeline to tune a model hyper-parameters with 3 fold cross validate. I tested many different models, including K-Nearest Neighbors, Logistic Regression, Random Forest, Support Vector Machines, Gaussian Naive Bayes and Extreme Gradient Boosting. 

K-Nearest Neighbors, Logistic Regression and Random Forest all did quite well. But I chose Logistic Regression because it gave me the best cross val score and the minimum FP and FN. 

### Conclusion
I liked working on this project, it gave me hands on experince on supervised learning and number of modeling classification techniques. However, if I'm going to do medical diagnostic classification in the future, I would make sure that most of the features have number of occurrences something happen per week or month rather than yes or no answers, I think that would give a better classification.
