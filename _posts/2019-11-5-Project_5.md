---
layout: post
title: Arabic Printed Text Image Recognition using CRNN
---


### Introduction
Arabic language is considered the fifth spoken language in the world,  yet not a lot of research has been done in recognizing text from printed documents which also known as Optical Character Recognition (OCR). 
Also, there is a huge need in digitizing historical documents where a lot of useful books and literatures are not being used because they are not being digitized [1].

Building a neural network model to recognize arabic text can be a bit challenging, and the difficulty lies in the multi-sequence based text, which is different from detecting one object with only one label.
Another challenge I have faced when dealing with sequence-like objects is that their length vary drastically. In this case the most suited neural network model is RNN, since CNN cannot be directly applied to sequence prediction, because these models often operate on inputs and outputs with fixed dimensions, and thus are incapable of producing a variable-length label sequence.
[2].

In this project, I went through many trials and errors that helped me land on a good solution, which is using the CRNN with CTC loss function. 

### Collecting Dataset
I requested my dataset from [Arabic Printed Text Dataset](https://diuf.unifr.ch/main/diva/APTI/), and they kindly gave me access to their data with 45 million images and their corresponding ground truth presented in an XML file.


### Preprocessing Data
Before I can use the images and their labels there is a preprocessing steps that have to be done:
* Convert each image into a gray scale
* Resize each image into (15,50)
* Expand image dimension as (15,50,1) to make it compatible with the input shape of architecture.
* Normalize the image pixel values by dividing it with 255.


### Creating Network Architecture

![](images/model_arch.PNG)

I have used the same architecture proposed in [2].

This CRNN network architecture consists of three layesrs. Strating from the bottom, the convolutional layers automatically
extract a feature sequence from each input image. then the recurrent networks are built for making prediction for each frame of the feature
sequence, outputted by the convolutional layers. Finally, the transcription layer is used to translate the per-frame predictions by the recurrent layers into a label sequence.





### References 
1. Jain, Mohit, et al. “Unconstrained OCR for Urdu Using Deep CNN-RNN Hybrid Networks.” 2017 4th IAPR Asian Conference on Pattern Recognition (ACPR), 2017, doi:10.1109/acpr.2017.5.

2. Shi, B., Bai, X., & Yao, C. (2017). An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(11), 2298–2304. doi: 10.1109/tpami.2016.2646371 
